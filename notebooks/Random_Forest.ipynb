{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "regular-trail",
   "metadata": {},
   "source": [
    "##### Note that for Random Forest without standaridization, the train/test periods doens't matter in the feature generating part (since there's no pca involved). We will identify them later in the code.\n",
    "generate_featureset('A', 'rank', 'csv', scale = None, roll_avg = None, pca = None, shift = True, train_periods = [20162, 20171, 20172, 20181, 20182, 20191, 20192], test_periods = [20201], ties = 'min', geo = False, spatial = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-seating",
   "metadata": {},
   "source": [
    "Best params for 1st round of testing (test on 2020.1, train on 2017.1-2019.1 & val on 2019.2): \n",
    "max_depth: 3\n",
    "max_features: 10\n",
    "min_samples_leaf: 20\n",
    "n_estimators: 30\n",
    "\n",
    "val acc = 42.199999999999996%\n",
    "test acc = 39.6%\n",
    "\n",
    "Best params for 2st round of testing (test on 2019.2, train on 2017.1-2018.2 & val on 2019.1): \n",
    "max_depth: 3\n",
    "max_features: 11\n",
    "min_samples_leaf: 20\n",
    "n_estimators: 30\n",
    "\n",
    "val acc = 41.6%\n",
    "test acc = 40.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "brazilian-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import scipy\n",
    "import pickle\n",
    "from CUSP_functions import generate_featureset\n",
    "from CUSP_functions import evaluator\n",
    "\n",
    "# for random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# make_score is for incorportating the lc20 as gridsearch eval metric\n",
    "from sklearn.metrics import r2_score,  make_scorer\n",
    "# kfold is for feature selection; predefined split is for gridsearch (cuz it's faster)\n",
    "from sklearn.model_selection import GridSearchCV, KFold, PredefinedSplit, cross_val_score\n",
    "# pipeline is for building pipeline to feed into gridsearch\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# set display max so that you can see as many rows/cols\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sustainable-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_periods = [20162, 20171, 20172, 20181, 20182, 20191, 20192, 20201]\n",
    "\n",
    "# because we will do t-1 and t-2 features to predict t overdose rank, so we start with 20171 ~ 20161 & 20162 features\n",
    "first_round_train_val_periods = [20171, 20172, 20181, 20182, 20191, 20192]\n",
    "first_round_test_period = [20201]\n",
    "\n",
    "second_round_train_val_periods = [20171, 20172, 20181, 20182, 20191]\n",
    "second_round_test_period = [20192]\n",
    "\n",
    "t_minus2 = all_periods[0:-1]\n",
    "t_minus1 = all_periods[1:]\n",
    "t_transform = dict(zip(t_minus2, t_minus1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "military-rhythm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_v4_0726_164842_A_rank____shift_min.csv\n",
      "total_features: 144\n"
     ]
    }
   ],
   "source": [
    "# read in the dataset\n",
    "# full = pd.read_csv(\"data/dataset_v4_0715_084729_A_rank____shift_min.csv\")\n",
    "full = generate_featureset('A', 'rank', 'csv', scale = None, roll_avg = None, pca = None, shift = True, train_periods = all_periods, test_periods = [], ties = 'min', geo = False, spatial = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "early-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep the rf main df\n",
    "full_rf = full.copy()\n",
    "\n",
    "# read the same df for prep t-2 features\n",
    "# in that case y of 20171 ~ x of 20162 and 20161\n",
    "# so our periods for modeling would be 20171 - 20201\n",
    "# in the prepared dataset, if one record is 20171, bc it's shifted, the features are already 20162's\n",
    "# therefore, we need to pull 20161 features from record marked as 20162\n",
    "full_2 = full.copy()\n",
    "# the features pulled through this process are actually 20162 - 20191\n",
    "full_t_previous = full_2[full_2.full_period.isin(t_minus2)]\n",
    "# add suffix for those features to distinguish them\n",
    "full_t_previous = full_t_previous.add_suffix('_t-2')\n",
    "# only keep from geoid_t-1 to overdose_rank_t-1 - actually this drops the year_t-1 and period_t-1 columns\n",
    "full_t_previous = full_t_previous.loc[:, 'geoid_t-2':'overdose_rank_t-2']\n",
    "# rename the geoid and full_period column, and we will join the t-1 df to main df using those two columns as keys\n",
    "full_t_previous = full_t_previous.rename(columns = {'geoid_t-2': 'geoid', 'full_period_t-2': 'full_period'})\n",
    "# manually shift period by + 1 aka. the original record marked as 20162, which means x is from 20161, will be shifted as 20171\n",
    "# in that way, the record marked as 20171 will have the features from 20161 (this is the t-2)\n",
    "full_t_previous['full_period'].replace(t_transform, inplace = True)\n",
    "\n",
    "# join to the main df\n",
    "full_rf = full_rf.merge(full_t_previous, on = ['geoid', 'full_period'], how = 'left')\n",
    "full_rf = full_rf[full.full_period.isin(t_minus1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "jewish-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set y variable as the normalized rank\n",
    "full_rf['y'] = full_rf['overdose_rank']\n",
    "# create index based on period and geoid\n",
    "full_rf['period_geoid'] = full_rf['full_period'].astype(str) + '_' +  full_rf['geoid'].astype(str)\n",
    "# this is going to be passed into the custom scorer as the \"signal\" - period and geoid\n",
    "full_rf.set_index('period_geoid', inplace = True)\n",
    "# to make sure each period's record stay together for the kfold\n",
    "full_rf.sort_index(ascending = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beneficial-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('RandomForest', RandomForestRegressor(random_state = 1234))])\n",
    "# put all the parameters that you would like to go through in gridsearch \n",
    "parameters = {'RandomForest__max_features': ['auto', 10, 11],\n",
    "              'RandomForest__max_depth': range(3, 8),\n",
    "              'RandomForest__min_samples_leaf': [20, 50, 100],\n",
    "              'RandomForest__n_estimators': [30, 40, 50]}\n",
    "\n",
    "\n",
    "# here it's the custimzed scorer - we need to wrap it up for feeding into gridsearch\n",
    "def lc20_scorer(y_true, y_pred): \n",
    "    # y, y_pred\n",
    "    # pull the period from the validation set\n",
    "    period = int(list(y_true.index)[0][0:5])\n",
    "    # pull geoids from the validation set\n",
    "    geoid = [int(idx[6:]) for idx in list(y_true.index)]\n",
    "     # call the eval function here to get the LC 20% capture\n",
    "    result, rest = evaluator(np.array(y_pred), np.array(geoid), period, target_var = 'rank', ties = 'min', simple = True)\n",
    "    return result.loc['20%', 'LC']\n",
    "\n",
    "lc20 = make_scorer(lc20_scorer, greater_is_better = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "insured-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "include = ['PDMP_total_persons_receiving_buprenorphine_at_least_7_days',\n",
    "'ACS_units_occupied_renter_pct_t-2',\n",
    "'EMS_total_count_t-2',\n",
    "'ACS_pop_hisp_pct',\n",
    "'PDMP_total_days_supply_buprenorphine_t-2',\n",
    "'PDMP_total_persons_receiving_buprenorphine_at_least_180_days',\n",
    "'EMS_total_count',\n",
    "'ACS_pop_hisp_white_alone_pct',\n",
    "'PDMP_total_mme_dispensed',\n",
    "'ACS_hh_med_income_t-2',\n",
    "'ACS_pop_hisp_pct_t-2',\n",
    "'PDMP_total_persons_initiating_buprenorphine_t-2',\n",
    "'ACS_pop_hisp_other_alone_pct_t-2',\n",
    "'segregation_classification_Predominantly Non-White_t-2',\n",
    "'ACS_pop_hisp_other_alone_pct',\n",
    "'PDMP_total_patients',\n",
    "'PDMP_total_persons_with_multiple_prescribers_or_dispensers_t-2',\n",
    "'ACS_pop_density_t-2',\n",
    "'PDMP_total_persons_initiating_buprenorphine',\n",
    "'ACS_pop_never_married_pct',\n",
    "'ACS_pop_inc_poverty_line_ratio_0_2',\n",
    "'ACS_hh_med_income',\n",
    "'EMS_age_35_44_t-2',\n",
    "'ACS_pop_hisp_white_alone_pct_t-2',\n",
    "'ACS_pop_inc_poverty_line_ratio_2_over_t-2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "devoted-subscriber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.048529517484463436\n",
      "first test: 42.199999999999996, 39.6, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.051151554217602646\n",
      "second test: 41.6, 40.8, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 40.2\n"
     ]
    }
   ],
   "source": [
    "# here it's looking through the top k features, and for each feature set, we do gridsearch twice to test on 20201 and 20192\n",
    "# we will use the feature set that has the best performances on average, and parameters accordingly\n",
    "\n",
    "# get the list of top k featues \n",
    "#include = include\n",
    "# first round of test: test 20201, train on 20162-20191 & val on 20192\n",
    "# preparing the train and val data frame for gridsearch\n",
    "X_train_val = full_rf[full_rf.full_period.isin(first_round_train_val_periods)].loc[:, full_rf.columns.isin(include)]\n",
    "y_train_val = full_rf[full_rf.full_period.isin(first_round_train_val_periods)].loc[:, 'y']\n",
    "\n",
    "# preparing the test data frame\n",
    "X_test = full_rf[full_rf.full_period.isin(first_round_test_period)].loc[:,  full_rf.columns.isin(include)]\n",
    "y_test_geoid = full_rf[full_rf.full_period.isin(first_round_test_period)].loc[:, 'geoid']\n",
    "y_test = full_rf[full_rf.full_period.isin(first_round_test_period)].loc[:, 'y']\n",
    "\n",
    "# use the predefined split to set 20171 - 20191 as training set (-1) and 20192 as validation set (1)\n",
    "split_index = [-1 if int(x[0:5]) < first_round_train_val_periods[-1] else 1 for x in X_train_val.index]\n",
    "cv_pd = PredefinedSplit(split_index)\n",
    "# gridsearch with lc20 as the scorer\n",
    "grid_search = GridSearchCV(pipeline, parameters, verbose = 2,  cv = cv_pd, n_jobs = 8, scoring = lc20)\n",
    "# get the best model using training with 20171 - 20191 and validating on 20192\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "# predict on 20201\n",
    "y_pred = grid_search.predict(X_test)\n",
    "# get the lc20 eval on 20201\n",
    "test_result_df, other = evaluator(np.array(y_pred), np.array(y_test_geoid), first_round_test_period[0], target_var = 'rank', ties = 'min', simple = True)\n",
    "test_result = test_result_df.loc['20%', 'LC']\n",
    "# print the result\n",
    "print(\"first test: \" +  str(grid_search.best_score_) + \", \" + str(test_result) +  \", params:\" + str(grid_search.best_params_))\n",
    "\n",
    "#second round of test: test 20192, train on 20162-20182 & val on 20191\n",
    "# preparing the train and val data frame for gridsearch\n",
    "X_train_val = full_rf[full_rf.full_period.isin(second_round_train_val_periods)].loc[:, full_rf.columns.isin(include)]\n",
    "y_train_val = full_rf[full_rf.full_period.isin(second_round_train_val_periods)].loc[:, 'y']\n",
    "# preparing the test data frame for gridsearch\n",
    "X_test = full_rf[full_rf.full_period.isin(second_round_test_period)].loc[:,  full_rf.columns.isin(include)]\n",
    "y_test_geoid = full_rf[full_rf.full_period.isin(second_round_test_period)].loc[:, 'geoid']\n",
    "y_test = full_rf[full_rf.full_period.isin(second_round_test_period)].loc[:, 'y']\n",
    "\n",
    "# use the predefined split to set 20171 - 20182 as training set (-1) and 20191 as validation set (1)\n",
    "split_index = [-1 if int(x[0:5]) < second_round_train_val_periods[-1] else 1 for x in X_train_val.index]\n",
    "cv_pd = PredefinedSplit(split_index)\n",
    "\n",
    "# gridsearch with lc20 as the scorer\n",
    "grid_search = GridSearchCV(pipeline, parameters, verbose = 2,  cv = cv_pd, n_jobs = 8, scoring = lc20)\n",
    "\n",
    "# get the best model using training with 20171 - 20182 and validating on 20191\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "# predict on 20192\n",
    "y_pred = grid_search.predict(X_test)\n",
    "# get the lc20 eval on 20192\n",
    "test_result_2_df, other = evaluator(np.array(y_pred), np.array(y_test_geoid), second_round_test_period[0], target_var = 'rank', ties = 'min', simple = True)\n",
    "test_result_2 = test_result_2_df.loc['20%', 'LC']\n",
    "# print the result\n",
    "print(\"second test: \" +   str(grid_search.best_score_) + \", \" + str(test_result_2) +  \", params:\" + str(grid_search.best_params_))\n",
    "print(\"avg test = \" + str((test_result + test_result_2)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "metallic-synthesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.048529517484463436\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39.6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the first round of test - best model\n",
    "\n",
    "X_train_val = full_rf[full_rf.full_period.isin(first_round_train_val_periods)].loc[:, full_rf.columns.isin(include)]\n",
    "y_train_val = full_rf[full_rf.full_period.isin(first_round_train_val_periods)].loc[:, 'y']\n",
    "\n",
    "# preparing the test data frame\n",
    "X_test = full_rf[full_rf.full_period.isin(first_round_test_period)].loc[:,  full_rf.columns.isin(include)]\n",
    "y_test_geoid = full_rf[full_rf.full_period.isin(first_round_test_period)].loc[:, 'geoid']\n",
    "y_test = full_rf[full_rf.full_period.isin(first_round_test_period)].loc[:, 'y']\n",
    "\n",
    "reg = RandomForestRegressor(max_depth=3, max_features=10,\n",
    "                     min_samples_leaf=20, n_estimators=30,\n",
    "                     random_state=1234)\n",
    "reg.fit(X_train_val, y_train_val)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "test_result_df, other = evaluator(np.array(y_pred), np.array(y_test_geoid), first_round_test_period[0], target_var = 'rank', ties = 'min', simple = True)\n",
    "test_result = test_result_df.loc['20%', 'LC']\n",
    "\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "clinical-wrong",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.051151554217602646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40.8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the second round of test - best model\n",
    "\n",
    "X_train_val = full_rf[full_rf.full_period.isin(second_round_train_val_periods)].loc[:, full_rf.columns.isin(include)]\n",
    "y_train_val = full_rf[full_rf.full_period.isin(second_round_train_val_periods)].loc[:, 'y']\n",
    "\n",
    "# preparing the test data frame\n",
    "X_test = full_rf[full_rf.full_period.isin(second_round_test_period)].loc[:,  full_rf.columns.isin(include)]\n",
    "y_test_geoid = full_rf[full_rf.full_period.isin(second_round_test_period)].loc[:, 'geoid']\n",
    "y_test = full_rf[full_rf.full_period.isin(second_round_test_period)].loc[:, 'y']\n",
    "\n",
    "reg = RandomForestRegressor(max_depth=3, max_features=11,\n",
    "                     min_samples_leaf=20, n_estimators=30,\n",
    "                     random_state=1234)\n",
    "reg.fit(X_train_val, y_train_val)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "test_result_df, other = evaluator(np.array(y_pred), np.array(y_test_geoid), second_round_test_period[0], target_var = 'rank', ties = 'min', simple = True)\n",
    "test_result = test_result_df.loc['20%', 'LC']\n",
    "\n",
    "test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-straight",
   "metadata": {},
   "source": [
    "## Appendix: Finding best feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-wales",
   "metadata": {},
   "source": [
    "Please note that, for random forest, the model is not as stable as we would expect. The following process shows the work flow for finding the historical feature importances using the 2017.1-2018.2 data, and use top k most important features for GridSearch. The LC20% capture fluctuates 1-5% with one additional feature than the prior one. In addition, please note that when using LC20% for tuning, there could be ties in the result. The GridSearchCV function would pick up the one with the smallest index as the best parameter set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "convertible-apparel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 135 candidates, totalling 540 fits\n",
      "{'RandomForest__max_depth': 4, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "# this part is for feature importance ranking - use the train set of the 2nd round of test, dropping the validation period\n",
    "feature_importance_train_val_period = second_round_train_val_periods[0:-1]\n",
    "\n",
    "# list the columns that you will not use in the X_train\n",
    "not_include = ['year', 'period', 'geoid', 'full_period', 'period_geoid', 'overdose_rank', 'overdose_count','label', 'y', 'x_coord', 'y_coord']\n",
    "\n",
    "X_train_val = full_rf[full_rf.full_period.isin(feature_importance_train_val_period)].loc[:, ~full_rf.columns.isin(not_include)]\n",
    "y_train_val = full_rf[full_rf.full_period.isin(feature_importance_train_val_period)].loc[:, 'y']\n",
    "\n",
    "# setup kfold - no shuffle because we need records of one entire period to be fed into the lc20 scorer\n",
    "cv = KFold(n_splits = len(feature_importance_train_val_period), shuffle = False)\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, verbose = 2,  cv = cv, n_jobs = 8, scoring = lc20)\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "# check the best params\n",
    "params = list(grid_search.best_params_.values())\n",
    "print(str(grid_search.best_params_))\n",
    "\n",
    "reg = RandomForestRegressor(max_depth=params[0], max_features=params[1],\n",
    "                     min_samples_leaf=params[2], n_estimators=params[3],\n",
    "                     random_state=1234)\n",
    "reg.fit(X_train_val, y_train_val)\n",
    "\n",
    "# print the feature importances by descending order\n",
    "feature_importance_dictionary = dict(zip(list(X_train_val.columns), list(reg.feature_importances_)))\n",
    "highest_features = dict(sorted(feature_importance_dictionary.items(), key = lambda item:item[1], reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "crude-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "best_parameters = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "brazilian-sport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04595970126790039\n",
      "first train 12: 41.5, 39.6, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05468725750964132\n",
      "second train 12: 45.0, 38.1, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "avg test = 38.85\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04779098994628772\n",
      "first train 13: 42.199999999999996, 38.5, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.056533064529957655\n",
      "second train 13: 42.3, 40.8, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "avg test = 39.65\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.047848284943861064\n",
      "first train 14: 42.9, 37.4, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05528950562945456\n",
      "second train 14: 41.6, 42.9, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 40.15\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.053219126364008384\n",
      "first train 15: 42.9, 36.4, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.06037961116101909\n",
      "second train 15: 43.0, 42.9, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "avg test = 39.65\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04718239071336727\n",
      "first train 16: 41.5, 39.6, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05868858092718032\n",
      "second train 16: 43.6, 38.1, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 38.85\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04227714750861977\n",
      "first train 17: 43.5, 37.4, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05951885143251301\n",
      "second train 17: 43.0, 38.1, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 37.75\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04523484578074333\n",
      "first train 18: 42.199999999999996, 40.1, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 50}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05958890330733213\n",
      "second train 18: 42.3, 37.4, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 38.75\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.050822186085049625\n",
      "first train 19: 39.5, 39.6, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05701179207563367\n",
      "second train 19: 43.0, 36.7, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 38.150000000000006\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04612710284459387\n",
      "first train 20: 40.1, 39.6, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05474988760806965\n",
      "second train 20: 42.3, 34.0, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 36.8\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04293720649616417\n",
      "first train 21: 40.1, 39.6, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.0476281025756744\n",
      "second train 21: 42.3, 32.7, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 36.150000000000006\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04233545380535808\n",
      "first train 22: 41.5, 36.9, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05392513625575002\n",
      "second train 22: 41.6, 34.699999999999996, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 30}\n",
      "avg test = 35.8\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.047187158902177506\n",
      "first train 23: 40.8, 39.0, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 50}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.057129340514571636\n",
      "second train 23: 41.6, 38.1, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "avg test = 38.55\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05076607521782617\n",
      "first train 24: 42.199999999999996, 38.0, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05309690266269784\n",
      "second train 24: 42.3, 39.5, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "avg test = 38.75\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.048529517484463436\n",
      "first train 25: 42.199999999999996, 39.6, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.051151554217602646\n",
      "second train 25: 41.6, 40.8, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 40.2\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.045276629132886215\n",
      "first train 26: 40.1, 35.8, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04765212568986088\n",
      "second train 26: 40.9, 36.1, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 35.95\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04979571967966545\n",
      "first train 27: 42.199999999999996, 40.1, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 50}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05118859686449939\n",
      "second train 27: 42.3, 35.4, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "avg test = 37.75\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04524473256845962\n",
      "first train 28: 41.5, 39.0, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05205262926587828\n",
      "second train 28: 42.3, 39.5, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 40}\n",
      "avg test = 39.25\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04718137275384604\n",
      "first train 29: 40.1, 40.6, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04760858599641349\n",
      "second train 29: 43.0, 35.4, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "avg test = 38.0\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.041892466163419906\n",
      "first train 30: 38.1, 39.0, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05668648482315697\n",
      "second train 30: 43.6, 37.4, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 38.2\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.046673135689662026\n",
      "first train 31: 39.5, 40.1, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.045187315829025576\n",
      "second train 31: 43.0, 35.4, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 37.75\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04541247626100309\n",
      "first train 32: 40.1, 40.6, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04937651037283197\n",
      "second train 32: 43.0, 36.7, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 38.650000000000006\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05034945797678536\n",
      "first train 33: 40.8, 38.5, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.049253616868465744\n",
      "second train 33: 43.0, 36.7, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 37.6\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.046973036638416565\n",
      "first train 34: 41.5, 38.5, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 50}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05681324931796383\n",
      "second train 34: 43.0, 37.4, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 37.95\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.049679979401784036\n",
      "first train 35: 41.5, 38.5, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05393417966416969\n",
      "second train 35: 43.6, 38.1, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 30}\n",
      "avg test = 38.3\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04621433000722874\n",
      "first train 36: 39.5, 38.5, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05720183254113331\n",
      "second train 36: 43.0, 34.699999999999996, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "avg test = 36.599999999999994\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04600312074349211\n",
      "first train 37: 40.1, 38.5, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 50}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.0612612026847037\n",
      "second train 37: 43.0, 34.0, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "avg test = 36.25\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05058129615287288\n",
      "first train 38: 42.199999999999996, 39.6, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.0615751939649255\n",
      "second train 38: 42.3, 34.699999999999996, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "avg test = 37.15\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04088659860499033\n",
      "first train 39: 39.5, 35.8, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.059122641204664594\n",
      "second train 39: 42.3, 36.7, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 36.25\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05142398945384408\n",
      "first train 40: 40.1, 39.0, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05923726676633423\n",
      "second train 40: 43.0, 34.699999999999996, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "avg test = 36.849999999999994\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.040309811018604425\n",
      "first train 41: 38.800000000000004, 36.4, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.056446431900883054\n",
      "second train 41: 43.0, 34.699999999999996, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "avg test = 35.55\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.043339024195332754\n",
      "first train 42: 38.1, 33.7, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05258972585134791\n",
      "second train 42: 41.6, 33.300000000000004, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 40}\n",
      "avg test = 33.5\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.051056644517056826\n",
      "first train 43: 41.5, 39.0, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05593332562237385\n",
      "second train 43: 42.3, 36.1, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "avg test = 37.55\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04143357948214532\n",
      "first train 44: 38.1, 40.1, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05580829438753265\n",
      "second train 44: 42.3, 36.1, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "avg test = 38.1\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.03338888429801079\n",
      "first train 45: 39.5, 34.2, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05271997601427891\n",
      "second train 45: 42.3, 35.4, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "avg test = 34.8\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04889778509288423\n",
      "first train 46: 40.8, 38.5, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.050928096953531066\n",
      "second train 46: 42.3, 35.4, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "avg test = 36.95\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04881172730341621\n",
      "first train 47: 38.1, 34.8, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.050541002594529316\n",
      "second train 47: 40.9, 34.0, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "avg test = 34.4\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.0488501862193127\n",
      "first train 48: 39.5, 36.9, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05251735301239191\n",
      "second train 48: 40.9, 36.1, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "avg test = 36.5\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04743726145781413\n",
      "first train 49: 42.9, 35.3, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05153011912341998\n",
      "second train 49: 40.9, 36.1, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 40}\n",
      "avg test = 35.7\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.0448967686986218\n",
      "first train 50: 39.5, 35.8, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.049281402592172396\n",
      "second train 50: 44.3, 34.699999999999996, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 30}\n",
      "avg test = 35.25\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.0367266605356702\n",
      "first train 51: 40.1, 33.2, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.052667189024183214\n",
      "second train 51: 41.6, 36.1, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "avg test = 34.650000000000006\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04630525506490302\n",
      "first train 52: 40.8, 36.4, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.057412934961506124\n",
      "second train 52: 40.300000000000004, 38.800000000000004, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 37.6\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04155640520449788\n",
      "first train 53: 40.1, 34.8, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.049399036541449615\n",
      "second train 53: 40.300000000000004, 36.7, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "avg test = 35.75\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.03581113497788457\n",
      "first train 54: 41.5, 35.8, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.052132353094706096\n",
      "second train 54: 41.6, 34.699999999999996, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 40}\n",
      "avg test = 35.25\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.041544887766048144\n",
      "first train 55: 42.9, 35.3, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05531419086073219\n",
      "second train 55: 40.300000000000004, 36.7, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "avg test = 36.0\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.042496143175249856\n",
      "first train 56: 41.5, 34.8, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.056425279297702136\n",
      "second train 56: 41.6, 35.4, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 50}\n",
      "avg test = 35.099999999999994\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.03492685409036367\n",
      "first train 57: 41.5, 33.7, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04851390902151009\n",
      "second train 57: 43.6, 34.0, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 30}\n",
      "avg test = 33.85\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.03989200398894832\n",
      "first train 58: 42.199999999999996, 34.8, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05038034147881609\n",
      "second train 58: 43.0, 34.0, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 30}\n",
      "avg test = 34.4\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04015500118404147\n",
      "first train 59: 44.9, 33.7, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05358038054092329\n",
      "second train 59: 43.0, 34.699999999999996, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 34.2\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.03855020557729294\n",
      "first train 60: 44.2, 36.4, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.06488583013619065\n",
      "second train 60: 41.6, 42.9, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 39.65\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.036250886638258595\n",
      "first train 61: 42.9, 32.6, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.06288477112410829\n",
      "second train 61: 40.9, 37.4, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 35.0\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.036077087089455895\n",
      "first train 62: 44.2, 33.7, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05510466891904131\n",
      "second train 62: 42.3, 35.4, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 34.55\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.038902912421497904\n",
      "first train 63: 42.9, 35.8, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04860587226548063\n",
      "second train 63: 40.9, 38.1, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 50}\n",
      "avg test = 36.95\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.03910173282100726\n",
      "first train 64: 42.9, 36.9, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05885987770773471\n",
      "second train 64: 40.9, 36.7, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "avg test = 36.8\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.03962980358125168\n",
      "first train 65: 42.9, 36.9, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05561760755989076\n",
      "second train 65: 42.3, 38.1, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 30}\n",
      "avg test = 37.5\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.037514970204553855\n",
      "first train 66: 43.5, 34.8, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05962759276058138\n",
      "second train 66: 43.6, 37.4, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 36.099999999999994\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.048598901462000965\n",
      "first train 67: 41.5, 38.0, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.056484938062117185\n",
      "second train 67: 43.0, 36.7, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "avg test = 37.35\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.045493579771088366\n",
      "first train 68: 40.8, 33.7, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05513039367328765\n",
      "second train 68: 41.6, 36.1, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "avg test = 34.900000000000006\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04130887319423493\n",
      "first train 69: 42.9, 36.9, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05168155678505271\n",
      "second train 69: 41.6, 36.1, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 30}\n",
      "avg test = 36.5\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.037276857061610214\n",
      "first train 70: 40.8, 34.8, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05097252242287853\n",
      "second train 70: 42.3, 34.0, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 30}\n",
      "avg test = 34.4\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04351480144558029\n",
      "first train 71: 40.1, 34.8, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.0572182378049344\n",
      "second train 71: 42.3, 37.4, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 30}\n",
      "avg test = 36.099999999999994\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.045113126348857446\n",
      "first train 72: 40.8, 33.2, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.06410467518892138\n",
      "second train 72: 43.6, 38.800000000000004, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "avg test = 36.0\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.0447422889338267\n",
      "first train 73: 42.199999999999996, 33.7, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04530675966420583\n",
      "second train 73: 43.6, 33.300000000000004, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 30}\n",
      "avg test = 33.5\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.044638331558651934\n",
      "first train 74: 42.199999999999996, 33.2, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05727135872551725\n",
      "second train 74: 42.3, 34.699999999999996, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "avg test = 33.95\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.0434534645241309\n",
      "first train 75: 41.5, 35.3, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05953770456119267\n",
      "second train 75: 42.3, 36.7, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 36.0\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04465338606692726\n",
      "first train 76: 42.199999999999996, 35.3, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05331768627342204\n",
      "second train 76: 43.6, 32.7, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 30}\n",
      "avg test = 34.0\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.044899487435073104\n",
      "first train 77: 41.5, 34.2, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05592307901793192\n",
      "second train 77: 41.6, 35.4, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 34.8\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04455230085790385\n",
      "first train 78: 41.5, 34.2, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05283853294500118\n",
      "second train 78: 41.6, 32.7, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 30}\n",
      "avg test = 33.45\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04640044155875345\n",
      "first train 79: 40.8, 38.0, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05182294110615904\n",
      "second train 79: 42.3, 34.0, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 40}\n",
      "avg test = 36.0\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04445816079001175\n",
      "first train 80: 40.1, 35.3, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05026741636223786\n",
      "second train 80: 43.0, 33.300000000000004, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 30}\n",
      "avg test = 34.3\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.043887288708922445\n",
      "first train 81: 41.5, 34.8, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04694841935199612\n",
      "second train 81: 42.3, 35.4, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 30}\n",
      "avg test = 35.099999999999994\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04372852066425459\n",
      "first train 82: 40.1, 34.8, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05961050878394225\n",
      "second train 82: 43.0, 37.4, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 36.099999999999994\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.044400443472441875\n",
      "first train 83: 40.8, 33.7, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05327670696755682\n",
      "second train 83: 42.3, 36.1, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 30}\n",
      "avg test = 34.900000000000006\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.044400443472441875\n",
      "first train 84: 40.8, 33.7, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05205494451827408\n",
      "second train 84: 43.6, 34.0, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "avg test = 33.85\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04455736263863874\n",
      "first train 85: 42.199999999999996, 33.7, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.057919967708002895\n",
      "second train 85: 41.6, 36.1, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "avg test = 34.900000000000006\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04423120308465145\n",
      "first train 86: 41.5, 34.2, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05056530790000591\n",
      "second train 86: 43.0, 34.699999999999996, params:{'RandomForest__max_depth': 7, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 30}\n",
      "avg test = 34.45\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04403055640689757\n",
      "first train 87: 41.5, 34.2, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05381677400188023\n",
      "second train 87: 43.0, 34.699999999999996, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 30}\n",
      "avg test = 34.45\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04417340116325863\n",
      "first train 88: 41.5, 34.2, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05201471079241449\n",
      "second train 88: 40.9, 36.7, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 40}\n",
      "avg test = 35.45\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04403297529999328\n",
      "first train 89: 40.8, 33.7, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04542162374143721\n",
      "second train 89: 40.9, 34.0, params:{'RandomForest__max_depth': 3, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 50}\n",
      "avg test = 33.85\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04467259415574709\n",
      "first train 90: 42.199999999999996, 33.7, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.052085954264046386\n",
      "second train 90: 41.6, 34.699999999999996, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "avg test = 34.2\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04432858065062495\n",
      "first train 91: 40.8, 35.3, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04960438607891604\n",
      "second train 91: 43.6, 33.300000000000004, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 40}\n",
      "avg test = 34.3\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04427296425127092\n",
      "first train 92: 40.8, 34.2, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05347484299011296\n",
      "second train 92: 42.3, 33.300000000000004, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 50}\n",
      "avg test = 33.75\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04410376999196941\n",
      "first train 93: 40.8, 34.8, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 50}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.051960293155438264\n",
      "second train 93: 43.6, 38.1, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 30}\n",
      "avg test = 36.45\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04356476280134758\n",
      "first train 94: 40.8, 35.3, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04907767738653046\n",
      "second train 94: 43.0, 35.4, params:{'RandomForest__max_depth': 6, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 30}\n",
      "avg test = 35.349999999999994\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.043599717574180796\n",
      "first train 95: 40.1, 35.3, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05166236056254869\n",
      "second train 95: 43.0, 35.4, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 30}\n",
      "avg test = 35.349999999999994\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.044194234296614954\n",
      "first train 96: 40.8, 35.3, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05612303571705346\n",
      "second train 96: 43.0, 34.0, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "avg test = 34.65\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04414764801317561\n",
      "first train 97: 40.8, 35.3, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.0551536140488631\n",
      "second train 97: 43.0, 35.4, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 30}\n",
      "avg test = 35.349999999999994\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.0439879061431252\n",
      "first train 98: 40.8, 35.3, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.04888036537414342\n",
      "second train 98: 41.6, 34.699999999999996, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 100, 'RandomForest__n_estimators': 40}\n",
      "avg test = 35.0\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.043970757157149265\n",
      "first train 99: 40.8, 35.8, params:{'RandomForest__max_depth': 4, 'RandomForest__max_features': 'auto', 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 40}\n",
      "Fitting 1 folds for each of 135 candidates, totalling 135 fits\n",
      "0.05237339279968578\n",
      "second train 99: 43.6, 38.1, params:{'RandomForest__max_depth': 5, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 50, 'RandomForest__n_estimators': 30}\n",
      "avg test = 36.95\n"
     ]
    }
   ],
   "source": [
    "# here it's looking through the top k features, and for each feature set, we do gridsearch twice to test on 20201 and 20192\n",
    "# we will use the feature set that has the best performances on average, and parameters accordingly\n",
    "for k in range(12, 100):\n",
    "    # get the list of top k featues \n",
    "    include = list(highest_features.keys())[0:k]\n",
    "    # first round of test: test 20201, train on 20162-20191 & val on 20192    \n",
    "    # preparing the train and val data frame for gridsearch\n",
    "    X_train_val = full_rf[full_rf.full_period.isin(first_round_train_val_periods)].loc[:, full_rf.columns.isin(include)]\n",
    "    y_train_val = full_rf[full_rf.full_period.isin(first_round_train_val_periods)].loc[:, 'y']\n",
    "    \n",
    "    # preparing the test data frame\n",
    "    X_test = full_rf[full_rf.full_period.isin(first_round_test_period)].loc[:,  full_rf.columns.isin(include)]\n",
    "    y_test_geoid = full_rf[full_rf.full_period.isin(first_round_test_period)].loc[:, 'geoid']\n",
    "    y_test = full_rf[full_rf.full_period.isin(first_round_test_period)].loc[:, 'y']\n",
    "    \n",
    "    # use the predefined split to set 20171 - 20191 as training set (-1) and 20192 as validation set (1)\n",
    "    split_index = [-1 if int(x[0:5]) < first_round_train_val_periods[-1] else 1 for x in X_train_val.index]\n",
    "    cv_pd = PredefinedSplit(split_index)\n",
    "    # gridsearch with lc20 as the scorer\n",
    "    grid_search = GridSearchCV(pipeline, parameters, verbose = 2,  cv = cv_pd, n_jobs = 8, scoring = lc20)\n",
    "    # get the best model using training with 20171 - 20191 and validating on 20192\n",
    "    grid_search.fit(X_train_val, y_train_val)\n",
    "    # predict on 20201\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    # get the lc20 eval on 20201\n",
    "    test_result_df, other = evaluator(np.array(y_pred), np.array(y_test_geoid), first_round_test_period[0], target_var = 'rank', ties = 'min', simple = True)\n",
    "    test_result = test_result_df.loc['20%', 'LC']\n",
    "    best_params = str(grid_search.best_params_)\n",
    "    # print the result\n",
    "    print(\"first train \" + str(k) + \": \"+  str(grid_search.best_score_) + \", \" + str(test_result) +  \", params:\" + best_params)\n",
    "    \n",
    "    #second round of test: test 20192, train on 20162-20182 & val on 20191\n",
    "    # preparing the train and val data frame for gridsearch\n",
    "    X_train_val = full_rf[full_rf.full_period.isin(second_round_train_val_periods)].loc[:, full_rf.columns.isin(include)]\n",
    "    y_train_val = full_rf[full_rf.full_period.isin(second_round_train_val_periods)].loc[:, 'y']\n",
    "    # preparing the test data frame for gridsearch\n",
    "    X_test = full_rf[full_rf.full_period.isin(second_round_test_period)].loc[:,  full_rf.columns.isin(include)]\n",
    "    y_test_geoid = full_rf[full_rf.full_period.isin(second_round_test_period)].loc[:, 'geoid']\n",
    "    y_test = full_rf[full_rf.full_period.isin(second_round_test_period)].loc[:, 'y']\n",
    "    \n",
    "    # use the predefined split to set 20171 - 20182 as training set (-1) and 20191 as validation set (1)\n",
    "    split_index = [-1 if int(x[0:5]) < second_round_train_val_periods[-1] else 1 for x in X_train_val.index]\n",
    "    cv_pd = PredefinedSplit(split_index)\n",
    "    \n",
    "    # gridsearch with lc20 as the scorer\n",
    "    grid_search = GridSearchCV(pipeline, parameters, verbose = 2,  cv = cv_pd, n_jobs = 8, scoring = lc20)\n",
    "\n",
    "    # get the best model using training with 20171 - 20182 and validating on 20191\n",
    "    grid_search.fit(X_train_val, y_train_val)\n",
    "    # predict on 20192\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    # get the lc20 eval on 20192\n",
    "    test_result_2_df, other = evaluator(np.array(y_pred), np.array(y_test_geoid), second_round_test_period[0], target_var = 'rank', ties = 'min', simple = True)\n",
    "    test_result_2 = test_result_2_df.loc['20%', 'LC']\n",
    "    best_params_2 = str(grid_search.best_params_)\n",
    "    # print the result\n",
    "    print(\"second train \" + str(k) + \": \"+  str(grid_search.best_score_) + \", \" + str(test_result_2) +  \", params:\" + best_params_2)\n",
    "    print(\"avg test = \" + str((test_result + test_result_2)/2))\n",
    "    \n",
    "    # save the avg test result and best pramas of two rounds for review\n",
    "    result[k] = (test_result + test_result_2)/2\n",
    "    best_parameters[k] = \"First round best params: \" + best_params + \"; Second round best params: \" + best_params_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "disabled-genealogy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest test accuracy comes from 25 features.\n"
     ]
    }
   ],
   "source": [
    "highest_test = dict(sorted(result.items(), key = lambda item:item[1], reverse = True))\n",
    "print(\"The highest test accuracy comes from \" + str(list(highest_test.keys())[0]) + \" features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "electoral-grove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for two rounds are - First round best params: {'RandomForest__max_depth': 3, 'RandomForest__max_features': 10, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}; Second round best params: {'RandomForest__max_depth': 3, 'RandomForest__max_features': 11, 'RandomForest__min_samples_leaf': 20, 'RandomForest__n_estimators': 30}\n"
     ]
    }
   ],
   "source": [
    "# the four values are for: max_depth, max_features, min_samples_leaf, n_estimators, accordingly\n",
    "print(\"Best parameters for two rounds are - \" + best_parameters[list(highest_test.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "functioning-integer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PDMP_total_persons_receiving_buprenorphine_at_least_7_days',\n",
       " 'ACS_units_occupied_renter_pct_t-2',\n",
       " 'EMS_total_count_t-2',\n",
       " 'ACS_pop_hisp_pct',\n",
       " 'PDMP_total_days_supply_buprenorphine_t-2',\n",
       " 'PDMP_total_persons_receiving_buprenorphine_at_least_180_days',\n",
       " 'EMS_total_count',\n",
       " 'ACS_pop_hisp_white_alone_pct',\n",
       " 'PDMP_total_mme_dispensed',\n",
       " 'ACS_hh_med_income_t-2',\n",
       " 'ACS_pop_hisp_pct_t-2',\n",
       " 'PDMP_total_persons_initiating_buprenorphine_t-2',\n",
       " 'ACS_pop_hisp_other_alone_pct_t-2',\n",
       " 'segregation_classification_Predominantly Non-White_t-2',\n",
       " 'ACS_pop_hisp_other_alone_pct',\n",
       " 'PDMP_total_patients',\n",
       " 'PDMP_total_persons_with_multiple_prescribers_or_dispensers_t-2',\n",
       " 'ACS_pop_density_t-2',\n",
       " 'PDMP_total_persons_initiating_buprenorphine',\n",
       " 'ACS_pop_never_married_pct',\n",
       " 'ACS_pop_inc_poverty_line_ratio_0_2',\n",
       " 'ACS_hh_med_income',\n",
       " 'EMS_age_35_44_t-2',\n",
       " 'ACS_pop_hisp_white_alone_pct_t-2',\n",
       " 'ACS_pop_inc_poverty_line_ratio_2_over_t-2']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features that are used in the top k best performance\n",
    "include[0:list(highest_test.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-version",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
